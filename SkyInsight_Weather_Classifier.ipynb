{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "SkyInsight: Weather Classifier",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammedtaha78/World-Weather-Repository/blob/main/SkyInsight_Weather_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'weather-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F6087%2F8975%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241009%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241009T154839Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db60f4a1459c77906e8fb74322bf699543ae5508813656caa6295f66ab6a057a428554b64bcd463c82c4f1a264c057f9e22805267406335303c3db26bfc048bfa93bb65359df41e5db574d10f2ecbc15cbdf0c66894fac95af4cfe6749e30e72924650e98aca06cb6a26a32197249701264877fc94f243eb24015bccd49d71995b81450a75d2a8213ff125bd390277d22e09d08b03dc88e7cd39ca73ac4642683cd7fc8d6b84150c69a1c9339683531abe51dea2f28e18a5b34610b0132f602394f07503e5cf394f26af241a87fab3f720c3b348b2e983cd622c94a1c16473c9eab0fbc3556d25aae2d6a04f7664cf762e4568e258066d04c726e02e44a4891cd'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "SKDHjQpGA7Ss"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "02wwrNSOA7Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SkyInsight: AI-Powered Weather Forecasting System\n",
        "\n",
        "SkyInsight is a powerful system designed to accurately classify weather conditions as Overcast, Clear, or Foggy. This classifier employs advanced techniques from machine learning, deep learning, and transfer learning domains to achieve precise weather predictions.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <a href=\"https://images.unsplash.com/photo-1630260643564-7f9c9c140682?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1470&q=80\">\n",
        "       <img src=\"https://images.unsplash.com/photo-1630260643564-7f9c9c140682?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1470&q=80\" alt=\"Sunset at the Beach\" width=\"700\" height=\"500\">\n",
        "    </a>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "szKffuv_A7Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "sns.set_theme(style=\"dark\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-08T09:03:27.00903Z",
          "iopub.execute_input": "2023-07-08T09:03:27.00941Z",
          "iopub.status.idle": "2023-07-08T09:03:40.078914Z",
          "shell.execute_reply.started": "2023-07-08T09:03:27.009383Z",
          "shell.execute_reply": "2023-07-08T09:03:40.077726Z"
        },
        "trusted": true,
        "id": "hz5fTeDhA7Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions\n",
        "def remove_outliers(df, feature):\n",
        "    \"\"\"\n",
        "    Remove Outliers using IRQ method\n",
        "\n",
        "    df: dataframe\n",
        "    feature: dataframe column\"\"\"\n",
        "    q1 = df[feature].quantile(0.25)\n",
        "    q3 = df[feature].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    df = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
        "    return df\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,\n",
        "        sharex=True,\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 2, 6))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 2, 6))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n],\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )\n",
        "        else:\n",
        "            label = p.get_height()\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2\n",
        "        y = p.get_height()\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the distribution plot\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[2]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[2]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def checking_overfitting_undefitting(y_train, y_train_pred, y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Print whether the model is underfit, overfit or good fit.\n",
        "\n",
        "    y_train = training data\n",
        "    y_train_pred = predictions on training data\n",
        "    y_test = testing data\n",
        "    y_test_pred = predictions on testing data\n",
        "    \"\"\"\n",
        "    training_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    testing_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    if training_accuracy<=0.65:\n",
        "        print(\"Model is underfitting.\")\n",
        "    elif training_accuracy>0.65 and abs(training_accuracy-testing_accuracy)>0.15:\n",
        "        print(\"Model is overfitting.\")\n",
        "    else:\n",
        "        print(\"Model is not underfitting/overfitting.\")\n",
        "\n",
        "def calculate_classification_metrics(y_true, y_pred, algorithm):\n",
        "    \"\"\"\n",
        "    Return the classification Metrics\n",
        "\n",
        "    y_true = actual values\n",
        "    y_pred = predicted values\n",
        "    y_pred_probability = probability values\n",
        "    algorithm = algorithm name\n",
        "    \"\"\"\n",
        "    accuracy = round(accuracy_score(y_true, y_pred), 3)\n",
        "    precision = round(precision_score(y_true, y_pred, average='weighted'), 3)\n",
        "    recall = round(recall_score(y_true, y_pred, average='weighted'), 3)\n",
        "    f1 = round(f1_score(y_true, y_pred, average='weighted'), 3)\n",
        "    print(\"Algorithm: \", algorithm)\n",
        "    print()\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print()\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    labels = ['Overcast', 'Clear','Foggy']\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Callback function to avoid overfitting\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_accuracy')>0.90) and (logs.get('accuracy')>0.95):\n",
        "            print(\"\\nValidation and training accuracies are high so cancelling training!\")\n",
        "            self.model.stop_training = True\n"
      ],
      "metadata": {
        "id": "h0VcN2u3A7Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wPHMr6jEA7Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Inspection"
      ],
      "metadata": {
        "id": "E081w28xA7Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching data\n",
        "data = pd.read_csv(\"/kaggle/input/weather-dataset/weatherHistory.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "lfOWB-R3A7Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing the Data types and Exploring the number of entities in a feature\n",
        "data.info()"
      ],
      "metadata": {
        "id": "aCu1eY5wA7S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Statistical Summary\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "7llc_zZKA7S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Target Variable\n",
        "print(data[\"Summary\"].value_counts())"
      ],
      "metadata": {
        "id": "Knjxb1POA7S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduced Data (Using only 3 classes)\n",
        "data = data[(data[\"Summary\"] == \"Overcast\") | (data[\"Summary\"] == \"Clear\") | (data[\"Summary\"] == \"Foggy\")]\n",
        "data.info()"
      ],
      "metadata": {
        "id": "xGCM2lVNA7S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "0iADbKtcA7S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Cleaning"
      ],
      "metadata": {
        "id": "6g-5HrfLA7S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Missing Values Treatment"
      ],
      "metadata": {
        "id": "Q1ZPSC-FA7S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Missing Values\n",
        "missing_values_count = data.isnull().sum()\n",
        "missing_values_count"
      ],
      "metadata": {
        "id": "RQ5yUA6fA7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since 359 is a reasonable count. Dropping the respective rows. If the count were smaller we would've filled it up with dummy values\n",
        "data.dropna(inplace=True)\n",
        "# Again checking for values\n",
        "missing_values_count = data.isnull().sum()\n",
        "missing_values_count"
      ],
      "metadata": {
        "id": "ZRONTb5-A7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Duplicated Values Treatment"
      ],
      "metadata": {
        "id": "ze-j8iXPA7S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating number of duplicated values\n",
        "print(\"Duplicated Values: \",data.duplicated().sum())"
      ],
      "metadata": {
        "id": "Fhqg4tOXA7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicated values\n",
        "data.drop_duplicates(inplace=True)\n",
        "# Again checking for duplicated values\n",
        "print(\"Duplicated Values: \", data.duplicated().sum())"
      ],
      "metadata": {
        "id": "yIN0aUhCA7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Data Formatting"
      ],
      "metadata": {
        "id": "TELwWRYoA7S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rounding up the float64 data upto 2 decimals.\n",
        "float_cols = data.select_dtypes(include='float')\n",
        "data[float_cols.columns] = float_cols.round(2)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "caRHkKsuA7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting Date Column. This can be used to identify any seasonality and trends\n",
        "data['Formatted Date'] = pd.to_datetime(data['Formatted Date'], errors='coerce')\n",
        "# Extracting the relevant components\n",
        "data[\"Time\"] = [d.time() for d in data['Formatted Date']]\n",
        "data[\"Time\"] = data[\"Time\"].astype(str)\n",
        "data[\"Time\"] = data[\"Time\"].str.split(':').str[0].astype(int)\n",
        "data[\"Date\"] = [d.date() for d in data['Formatted Date']]\n",
        "data[\"Date\"]= data[\"Date\"].astype(str)\n",
        "data[\"Year\"] = data[\"Date\"].str.split('-').str[0].astype(int)\n",
        "data[\"Month\"] = data[\"Date\"].str.split('-').str[1].astype(int)\n",
        "data[\"Day\"] = data[\"Date\"].str.split('-').str[2].astype(int)\n",
        "# Dropping the original column\n",
        "data = data.drop(columns=['Formatted Date','Date'], axis=1)"
      ],
      "metadata": {
        "id": "WD2DkYADA7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Redundant Columns Treatment"
      ],
      "metadata": {
        "id": "4-doQ5EBA7S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It can be seen that the feature \"Loud Cover\" have only value '0' and mean and other statistical overview also support the deduction. Hence, it is the redundant column\n",
        "data[\"Loud Cover\"].value_counts()"
      ],
      "metadata": {
        "id": "sYIM4HAXA7S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'Loud Cover'\n",
        "data.drop(columns=[\"Loud Cover\"], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ouOKuQy3A7S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Outlier Treatment"
      ],
      "metadata": {
        "id": "MKbFQVQlA7S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different types of columns\n",
        "numeric_columns = list(data.select_dtypes(include=['float64', 'int64']).columns)\n",
        "categorical_columns = list(data.select_dtypes(include=['object']).columns)\n",
        "continuous_columns = [i for i in numeric_columns if len(list(data[i].unique()))>=25]\n",
        "discrete_columns = [i for i in numeric_columns if len(list(data[i].unique()))<25]\n",
        "print(\"Numerical Columns: \", numeric_columns)\n",
        "print()\n",
        "print(\"Categorical Columns: \", categorical_columns)\n",
        "print()\n",
        "print(\"Continuous Columns: \", continuous_columns)\n",
        "print()\n",
        "print(\"Discrete Columns: \", discrete_columns)"
      ],
      "metadata": {
        "id": "wiI6pd8DA7S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting for outliers in the data.\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.boxplot(data=data[numeric_columns])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h0vwdRtlA7S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Outliers\n",
        "data = remove_outliers(data, \"Pressure (millibars)\")\n",
        "data = remove_outliers(data, \"Wind Speed (km/h)\")\n",
        "data = remove_outliers(data, \"Humidity\")\n",
        "data = remove_outliers(data, \"Temperature (C)\")\n",
        "data = remove_outliers(data, \"Apparent Temperature (C)\")\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.boxplot(data=data[numeric_columns])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1l366K4OA7S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "cc79dMibA7S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "OZUMhzchA7S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Checking skewness of the numerical features"
      ],
      "metadata": {
        "id": "CIeCRihVA7S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical columns analysis\n",
        "for i in numeric_columns:\n",
        "    histogram_boxplot(data,i)"
      ],
      "metadata": {
        "id": "wGoESLdBA7S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Checking distribution of categorical features(Summary and Precip Type)"
      ],
      "metadata": {
        "id": "1tBw7D0fA7S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical columns analysis\n",
        "for i in categorical_columns:\n",
        "    if i in ['Daily Summary','Time']:\n",
        "        pass\n",
        "    else:\n",
        "        labeled_barplot(data, i)"
      ],
      "metadata": {
        "id": "US1an3q8A7S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Creating Word Cloud for 'daily summary'"
      ],
      "metadata": {
        "id": "c_2vmsSIA7S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Word Cloud for daily summary\n",
        "text = ' '.join(data['Daily Summary'].astype(str))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6edtQNXRA7S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Relations between numerical features & Target variable \"Summary\""
      ],
      "metadata": {
        "id": "x2TAxfQgA7S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multivariate analysis\n",
        "for i in numeric_columns:\n",
        "    distribution_plot_wrt_target(data, i, \"Summary\")"
      ],
      "metadata": {
        "id": "9gTbzTfCA7TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Relations between categorical features & Target variable \"Summary\""
      ],
      "metadata": {
        "id": "vi9USl5xA7TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked barplot\n",
        "stacked_barplot(data,\"Precip Type\" , 'Summary')"
      ],
      "metadata": {
        "id": "vahOC7n0A7TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ap3CAmvzA7TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Pre-processing & Feature Engineering"
      ],
      "metadata": {
        "id": "IBF5J1_6A7TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Classes Distribution"
      ],
      "metadata": {
        "id": "ycbtQFa9A7TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking whether the target variable is balanced or unbalanced\n",
        "counts = data[\"Summary\"].value_counts()\n",
        "total = counts.sum()\n",
        "percentages = (counts / total) * 100\n",
        "print(percentages)\n",
        "print()\n",
        "print(\"The classes are satifactory balanced\")"
      ],
      "metadata": {
        "id": "e1-QUOBEA7TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Dataset Split"
      ],
      "metadata": {
        "id": "Tlc2zhbpA7TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input features dataset\n",
        "input_df = data.drop(columns=\"Summary\", axis=1)\n",
        "input_df.head()"
      ],
      "metadata": {
        "id": "or92Q9ZNA7TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable\n",
        "# Applying mapping\n",
        "encoder = LabelEncoder()\n",
        "y = data[\"Summary\"]\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Checking the mapping of the classes\n",
        "class_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "for class_label, class_number in class_mapping.items():\n",
        "    print(f\"Class '{class_label}' is labeled as {class_number}\")"
      ],
      "metadata": {
        "id": "oPwnZNmUA7TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Categorical Features Encoding"
      ],
      "metadata": {
        "id": "t0qFbJZWA7TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As \"Precip Type\" have only 2 values, hence applying binary encoding\n",
        "mapping = {'rain': 0, 'snow': 1}\n",
        "input_df['Precip Type'] = input_df['Precip Type'].map(mapping)"
      ],
      "metadata": {
        "id": "DAJ_HyNIA7TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As \"Daily Summary\" have 221 unique values, hence applying Frequency encoding\n",
        "# Creating a new column for frequency encoding and removing previous column\n",
        "input_df['Daily Summary Frequency'] = input_df['Daily Summary'].map(input_df['Daily Summary'].value_counts(normalize=True))\n",
        "input_df.drop(columns=['Daily Summary'], axis=1, inplace=True)\n",
        "# Checking data\n",
        "input_df.head()"
      ],
      "metadata": {
        "id": "_bXooCIPA7TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Checking Multicollinearity"
      ],
      "metadata": {
        "id": "yeTkWjr1A7TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming multicollinearity using heatmap\n",
        "sns.set(style=\"white\")\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(input_df.corr(), annot=True, cmap='coolwarm', linewidths=.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1A1Wm22MA7TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As Daily Summary Frequency has a negligible correlation with the other features and it is apparent that the final target is not going to be affected by this, removing it.\n",
        "input_df.drop(['Daily Summary Frequency'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "nzi1IFYEA7TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As VIF of Temperature (C) is the highest and Temperature is highly correlated with Apparent Temperature, removing it\n",
        "input_df.drop(['Temperature (C)'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "2-sPXe0OA7TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Train-Test Split"
      ],
      "metadata": {
        "id": "q6zWIgjWA7TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X input set\n",
        "X = input_df.values\n",
        "X"
      ],
      "metadata": {
        "id": "qs3IQzCbA7TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "6DWn19zeA7TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Normalizing Input Features"
      ],
      "metadata": {
        "id": "vwcIPO1qA7TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply scaling on the input_df DataFrame\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "joblib.dump(scaler, \"scaler.pkl\")"
      ],
      "metadata": {
        "id": "DPT3WId2A7TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Skwr3uxsA7TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Model Building"
      ],
      "metadata": {
        "id": "6ZFdHMNOA7TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Parametric Algorithm 1: Logistic Regression Classifier"
      ],
      "metadata": {
        "id": "nfb2Zx8xA7TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'solver': ['liblinear', 'saga'],\n",
        "              'multi_class':['ovr', 'multinomial'],\n",
        "              'C':[0.001, 0.01, 10.0],\n",
        "              'penalty': ['l1', 'l2']}\n",
        "# Model Creation and Training\n",
        "model_lr = LogisticRegression(n_jobs=-1)\n",
        "models_lr = GridSearchCV(estimator=model_lr, param_grid=parameters, cv=4)\n",
        "models_lr.fit(x_train, y_train)\n",
        "best_parameters = models_lr.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions for train\n",
        "best_model_lr = models_lr.best_estimator_\n",
        "y_pred_lr = best_model_lr.predict(x_train)\n",
        "# Predictions for test\n",
        "y_pred_lr_new = best_model_lr.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_lr, y_test, y_pred_lr_new)"
      ],
      "metadata": {
        "id": "LICfF3Q0A7TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_lr, precision_lr, recall_lr, f1_lr = calculate_classification_metrics(y_test, y_pred_lr_new, \"Logistic Regression\")"
      ],
      "metadata": {
        "id": "UlAqMpEuA7TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Parametric Algorithm 2: Gaussian Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "C5D_Rfn-A7TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'var_smoothing':[1e-9, 1e-8, 1e-10]}\n",
        "# Model Creation and Training\n",
        "model_nb = GaussianNB()\n",
        "models_nb = GridSearchCV(estimator=model_nb, param_grid=parameters, cv=4)\n",
        "models_nb.fit(x_train, y_train)\n",
        "best_parameters = models_nb.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on training data\n",
        "best_model_nb = models_nb.best_estimator_\n",
        "y_pred_nb = best_model_nb.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_nb_new = best_model_nb.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_nb, y_test, y_pred_nb_new)"
      ],
      "metadata": {
        "id": "rXswzNE-A7TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_nb, precision_nb, recall_nb, f1_nb = calculate_classification_metrics(y_test, y_pred_nb_new, \"Gaussian NB\")"
      ],
      "metadata": {
        "id": "bI5JqkktA7TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Parametric Algorithm 3: Support Vector Machine (SVM) Classifier"
      ],
      "metadata": {
        "id": "fnDe_FukA7TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'loss':['log_loss','perceptron','hinge','squared_epsilon_insensitive'],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'alpha':[0.001,0.01,0.0001],\n",
        "              'learning_rate':['optimal','adaptive','invscaling']}\n",
        "# Model Creation and Training\n",
        "model_svc = SGDClassifier()\n",
        "models_svc = GridSearchCV(estimator=model_svc, param_grid=parameters, cv=4)\n",
        "models_svc.fit(x_train, y_train)\n",
        "best_parameters = models_svc.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_svc = models_svc.best_estimator_\n",
        "y_pred_svc = best_model_svc.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_svc_new = best_model_svc.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_svc, y_test, y_pred_svc_new)"
      ],
      "metadata": {
        "id": "svwhoDM-A7TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_svc, precision_svc, recall_svc, f1_svc = calculate_classification_metrics(y_test, y_pred_svc_new, \"SVC\")"
      ],
      "metadata": {
        "id": "oi5U8ialA7TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Parametric Algorithm 4: SGD Classifier"
      ],
      "metadata": {
        "id": "3qvm6bOjA7TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'loss':['log_loss','perceptron','hinge','squared_epsilon_insensitive'],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'alpha':[0.001,0.01,0.0001],\n",
        "              'learning_rate':['optimal','adaptive','invscaling']}\n",
        "# Model Creation and Training\n",
        "model_sgd = SGDClassifier()\n",
        "models_sgd = GridSearchCV(estimator=model_sgd, param_grid=parameters, cv=4)\n",
        "models_sgd.fit(x_train, y_train)\n",
        "best_parameters = models_sgd.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_sgd = models_sgd.best_estimator_\n",
        "y_pred_sgd = best_model_sgd.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_sgd_new = best_model_sgd.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_sgd, y_test, y_pred_sgd_new)"
      ],
      "metadata": {
        "id": "t_I6QtrLA7TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_sgd, precision_sgd, recall_sgd, f1_sgd = calculate_classification_metrics(y_test, y_pred_sgd_new, \"SGD Classifier\")"
      ],
      "metadata": {
        "id": "pGcP1_k1A7TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 1: Decision Tree Classifier"
      ],
      "metadata": {
        "id": "zh7y-23QA7TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'criterion':['gini', 'entropy', 'log_loss'],\n",
        "              'max_depth': [None, 5, 10],\n",
        "              'min_samples_split': [None, 2, 5],\n",
        "              'splitter':['best','random']}\n",
        "# Model Creation and Training\n",
        "model_dt = DecisionTreeClassifier()\n",
        "models_dt = GridSearchCV(estimator=model_dt, param_grid=parameters, cv=4)\n",
        "models_dt.fit(x_train, y_train)\n",
        "best_parameters = models_dt.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_dt = models_dt.best_estimator_\n",
        "y_pred_dt = best_model_dt.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_dt_new = best_model_dt.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_dt, y_test, y_pred_dt_new)"
      ],
      "metadata": {
        "id": "npB97NUJA7TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_dt, precision_dt, recall_dt, f1_dt = calculate_classification_metrics(y_test, y_pred_dt_new, \"Decision Tree\")"
      ],
      "metadata": {
        "id": "7tqawAwFA7TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 2: K Nearest Neighbours Classifier"
      ],
      "metadata": {
        "id": "3SzCHNwbA7TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree','kd_tree','brute'],\n",
        "            'p': [1,2]}\n",
        "# Model Creation and Training\n",
        "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "models_knn = GridSearchCV(estimator=model_knn, param_grid=parameters, cv=4)\n",
        "models_knn.fit(x_train, y_train)\n",
        "best_parameters = models_knn.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_knn = models_knn.best_estimator_\n",
        "y_pred_knn = best_model_knn.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_knn_new = best_model_knn.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_knn, y_test, y_pred_knn_new)"
      ],
      "metadata": {
        "id": "N3FnRLN1A7TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_knn, precision_knn, recall_knn, f1_knn = calculate_classification_metrics(y_test, y_pred_knn_new, \"KNN\")"
      ],
      "metadata": {
        "id": "5OA3wa-nA7TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 3: Random Forest Classifier"
      ],
      "metadata": {
        "id": "JbC8O4qyA7TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'max_depth': [None, 5],\n",
        "            'class_weight': [None, 'balanced'],\n",
        "            'min_samples_split': [None, 2, 5]}\n",
        "# Model Creation and Training\n",
        "model_rf = RandomForestClassifier()\n",
        "models_rf = GridSearchCV(estimator=model_rf, param_grid=parameters, cv=4)\n",
        "models_rf.fit(x_train, y_train)\n",
        "best_parameters = models_rf.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_rf = models_rf.best_estimator_\n",
        "y_pred_rf = best_model_rf.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_rf_new = best_model_rf.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_rf, y_test, y_pred_rf_new)"
      ],
      "metadata": {
        "id": "ldBkvsttA7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_rf, precision_rf, recall_rf, f1_rf = calculate_classification_metrics(y_test, y_pred_rf_new, \"Random Forest\")"
      ],
      "metadata": {
        "id": "oNoyGuwFA7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 4: Extra Trees Classifier"
      ],
      "metadata": {
        "id": "Qb2SalssA7TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {'max_depth': [None, 5],\n",
        "            'class_weight': [None, 'balanced'],\n",
        "            'min_samples_split': [None, 2, 5],\n",
        "            'criterion':['gini','log_loss','entropy']}\n",
        "# Model Creation and Training\n",
        "model_et = ExtraTreesClassifier()\n",
        "models_et = GridSearchCV(estimator=model_et, param_grid=parameters, cv=4)\n",
        "models_et.fit(x_train, y_train)\n",
        "best_parameters = models_et.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on train data\n",
        "best_model_et = models_et.best_estimator_\n",
        "y_pred_et = best_model_et.predict(x_train)\n",
        "# Predictions on test data\n",
        "y_pred_et_new = best_model_et.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_et, y_test, y_pred_et_new)"
      ],
      "metadata": {
        "id": "sdGX7MvIA7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_et, precision_et, recall_et, f1_et = calculate_classification_metrics(y_test, y_pred_et_new, \"Extra Trees\")"
      ],
      "metadata": {
        "id": "ZkrcKaNEA7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 5: Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "4eneu2jEA7TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "# Model creation and training\n",
        "model_gb = GradientBoostingClassifier()\n",
        "models_gb = GridSearchCV(estimator=model_gb, param_grid=parameters, cv=4)\n",
        "models_gb.fit(x_train, y_train)\n",
        "best_parameters = models_gb.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions on the training data\n",
        "best_model_gb = models_gb.best_estimator_\n",
        "y_pred_gb = best_model_gb.predict(x_train)\n",
        "# Predictions on the test data\n",
        "y_pred_gb_new= best_model_gb.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_gb, y_test, y_pred_gb_new)"
      ],
      "metadata": {
        "id": "3mN9PLVYA7TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_gb, precision_gb, recall_gb, f1_gb = calculate_classification_metrics(y_test, y_pred_gb_new, \"Gradient Boosting Classifier\")"
      ],
      "metadata": {
        "id": "Zow1yRvSA7TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Non-Parametric Algorithm 6: Bagging Classifier"
      ],
      "metadata": {
        "id": "NgZ6W3keA7TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "parameters = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_samples': [0.5, 0.7, 0.9],\n",
        "    'max_features': [0.5, 0.7, 0.9]\n",
        "}\n",
        "# Model creation and training\n",
        "model_bagging = BaggingClassifier()\n",
        "models_bagging = GridSearchCV(estimator=model_bagging, param_grid=parameters, cv=4)\n",
        "models_bagging.fit(x_train, y_train)\n",
        "best_parameters = models_bagging.best_params_\n",
        "print(\"Best Hyperparameters:\", best_parameters)\n",
        "print()\n",
        "# Predictions for train\n",
        "best_model_bagging = models_bagging.best_estimator_\n",
        "y_pred_bagging = best_model_bagging.predict(x_train)\n",
        "# Predictions for test\n",
        "y_pred_bagging_new= best_model_bagging.predict(x_test)\n",
        "checking_overfitting_undefitting(y_train, y_pred_bagging, y_test, y_pred_bagging_new)"
      ],
      "metadata": {
        "id": "bi550cPHA7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics Calculation\n",
        "print(\"Testing Performance\")\n",
        "accuracy_bc, precision_bc, recall_bc, f1_bc = calculate_classification_metrics(y_test, y_pred_bagging_new, \"Bagging Classifier\")"
      ],
      "metadata": {
        "id": "rFqBAcXbA7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "print(\"Testing Performances for Machine Learning Algorithms\")\n",
        "result = pd.DataFrame({\"Algorithms\":['Logistic Regression', \"Gaussian Naive Bayes\", \"SVC\", \"SGD Classifier\", \"Decision Tree\", \"KNN\",\"Random Forest\", \"Extra Trees Classifier\", \"Bagging Classifier\",\"Gradient Boosting Classifier\"],\n",
        "                       \"Accuracy\":[accuracy_lr, accuracy_nb, accuracy_svc, accuracy_sgd, accuracy_dt, accuracy_knn, accuracy_rf, accuracy_et, accuracy_bc, accuracy_gb],\n",
        "                       \"Precision\":[precision_lr, precision_nb, precision_svc, precision_sgd, precision_dt, precision_knn, precision_rf, precision_et, precision_bc, precision_gb],\n",
        "                       \"Recall\":[recall_lr, recall_nb, recall_svc, recall_sgd, recall_dt, recall_knn, recall_rf, recall_et, recall_bc, recall_gb],\n",
        "                       \"F1 Score\":[f1_lr, f1_nb, f1_svc, f1_sgd, f1_dt, f1_knn, f1_rf, f1_et, f1_bc, f1_gb]}).set_index('Algorithms')\n",
        "result"
      ],
      "metadata": {
        "id": "yo_Mxd9MA7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving sklearn machine learning models\n",
        "models = [best_model_dt, best_model_lr, best_model_knn, best_model_et, best_model_nb, best_model_rf, best_model_sgd, best_model_svc, best_model_gb, best_model_bagging]\n",
        "names = [\"dt\",\"lr\",\"knn\",\"et\",\"nb\",\"rf\",\"sgd\",\"svc\",\"gb\",\"bg\"]\n",
        "for i in range(len(models)):\n",
        "    joblib.dump(models[i],names[i]+\".pkl\")"
      ],
      "metadata": {
        "id": "MdO_lQaYA7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "G7iAqTxEA7TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Neural Network Preprocessing"
      ],
      "metadata": {
        "id": "n0LDqpDVA7TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for ANN & RNN\n",
        "num_classes = 3\n",
        "epochs = 150\n",
        "input_dimension = x_train.shape[1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "_lYHgFlNA7TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting labels to one-hot encoded format\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "uUFi8htmA7TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping input data for RNN\n",
        "x_train_reshaped = np.expand_dims(x_train, axis=2)\n",
        "x_test_reshaped = np.expand_dims(x_test, axis=2)"
      ],
      "metadata": {
        "id": "d46de1whA7TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Neural Network 1: Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "phdRd5QKA7TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 1: 64-64-128-3 Feed Forward Neural Network\n",
        "# Defining the ANN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu', input_dim=input_dimension))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "\n",
        "# Training\n",
        "history = model.fit(x_train, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bItVxM58A7TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_0, test_accuracy_0 = model.evaluate(x_test, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_0)\n",
        "print('Test Accuracy:', test_accuracy_0)\n",
        "# Saving the model to a file\n",
        "model.save('FFNN.h5')"
      ],
      "metadata": {
        "id": "w2xywiDMA7TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 2: 32-256-3 Feed Forward Neural Network with 'relu' and 'softmax'\n",
        "# Defining the ANN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu', input_dim=input_dimension))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "# Training\n",
        "history = model.fit(x_train, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jw-NmFQ7A7TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_1, test_accuracy_1 = model.evaluate(x_test, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_1)\n",
        "print('Test Accuracy:', test_accuracy_1)\n"
      ],
      "metadata": {
        "id": "KhGWKFbeA7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 3: 32-256-3 Feed Forward Neural Network with 'sigmoid' and 'softmax'\n",
        "# Defining the ANN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(32, activation='sigmoid', input_dim=input_dimension))\n",
        "model.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "# Training\n",
        "history = model.fit(x_train, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "73Az99VIA7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_2, test_accuracy_2 = model.evaluate(x_test, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_2)\n",
        "print('Test Accuracy:', test_accuracy_2)"
      ],
      "metadata": {
        "id": "qO2PE_NaA7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### Neural Network 2: Recurrent Neural Network"
      ],
      "metadata": {
        "id": "JnbriX5CA7TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 1: 32-64-128-3 RNN with 'relu' and 'softmax'\n",
        "# Defining the RNN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.SimpleRNN(32, activation='relu', input_shape=(input_dimension, 1)))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "# Training\n",
        "history = model.fit(x_train_reshaped, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test_reshaped, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7LqgaqVA7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_3, test_accuracy_3 = model.evaluate(x_test_reshaped, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_3)\n",
        "print('Test Accuracy:', test_accuracy_3)"
      ],
      "metadata": {
        "id": "-CxAzdfzA7TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 2: 32-256-3 RNN with 'relu' and 'softmax'\n",
        "# Defining the RNN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.SimpleRNN(32, activation='relu', input_shape=(input_dimension, 1)))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "# Training\n",
        "history = model.fit(x_train_reshaped, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test_reshaped, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LlJWOZ_BA7TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_4, test_accuracy_4 = model.evaluate(x_test_reshaped, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_4)\n",
        "print('Test Accuracy:', test_accuracy_4)\n",
        "# Saving the model to a file\n",
        "model.save('RNN.h5')"
      ],
      "metadata": {
        "id": "D1uLfx5VA7TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture 3: 32-265-3 RNN with 'sigmoid', 'tanh and 'softmax'\n",
        "# Defining the RNN architecture\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.SimpleRNN(32, activation='relu', input_shape=(input_dimension, 1)))\n",
        "model.add(tf.keras.layers.Dense(256, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Stopping early to avoid overfitting\n",
        "stop_callback = myCallback()\n",
        "# Training\n",
        "history = model.fit(x_train_reshaped, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(x_test_reshaped, y_test_encoded), callbacks=[stop_callback])\n",
        "\n",
        "# Plotting training and testing curves\n",
        "default_size = plt.rcParams['figure.figsize']\n",
        "fig = plt.figure(figsize=[default_size[0] * 2, default_size[1]])\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label = 'accuracy')          # Train accuracy (blue)\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')  # Valid accuracy (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')          # Train loss (blue)\n",
        "plt.plot(history.history['val_loss'], label='val_loss')  # Valid loss (orange)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G1DA7tknA7TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data\n",
        "test_loss_5, test_accuracy_5 = model.evaluate(x_test_reshaped, y_test_encoded, verbose=0)\n",
        "print('Test Loss:', test_loss_5)\n",
        "print('Test Accuracy:', test_accuracy_5)"
      ],
      "metadata": {
        "id": "tLB-A3KkA7TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "print(\"Testing Performances for Deep Learning Algorithms on 150 Epochs\")\n",
        "result = pd.DataFrame({\"Algorithms\":['64-64-128-3 Feed Forward Neural Network', \"32-256-3 Feed Forward Neural Network\", \"32-256-3 Feed Forward Neural Network\", \"32-64-128-3 Recurrent Neural Network\", \"32-256-3 Recurrent Neural Network\", \"32-256-3 Recurrent Neural Network\"],\n",
        "                       \"Activation Functions\":[\"relu\", \"relu\",\"sigmoid\", \"relu\",\"relu\", \"relu & tanh\"],\n",
        "                       \"Optimizers\":[\"Adam\",\"SGD\",\"SGD\",\"Adam\",\"Adam\", \"RMSprop\"],\n",
        "                       \"Accuracy\":[round(test_accuracy_0,3), round(test_accuracy_1,3), round(test_accuracy_2,3), round(test_accuracy_3,3), round(test_accuracy_4,3), round(test_accuracy_5,3)],\n",
        "                       \"Loss\":[round(test_loss_0,3), round(test_loss_1,3), round(test_loss_2,3), round(test_loss_3,3), round(test_loss_4,3), round(test_loss_5,3)]}).set_index('Algorithms')\n",
        "result"
      ],
      "metadata": {
        "id": "VHVGB9PeA7TS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}